{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binary_Classification_NN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUVkd7W6VMAt"
      },
      "source": [
        "**Implementing deep neural network for performing binary classification task.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkY5jHSvX3DK"
      },
      "source": [
        "The dataset we will use in this is the Sonar dataset.\n",
        "\n",
        "This is a dataset that describes sonar chirp returns bouncing off different services. \n",
        "\n",
        "The 60 input variables are the strength of the returns at different angles. \n",
        "\n",
        "It is a binary classification problem that requires a model to differentiate rocks from metal cylinders.\n",
        "\n",
        "It is a well-understood dataset. \n",
        "\n",
        "All of the variables are continuous and generally in the range of 0 to 1. \n",
        "\n",
        "The output variable is a string “M” for mine and “R” for rock, which will need to be converted to integers 1 and 0.\n",
        "\n",
        "A benefit of using this dataset is that it is a standard benchmark problem. \n",
        "\n",
        "This means that we have some idea of the expected skill of a good model. \n",
        "\n",
        "Using cross-validation, a neural network should be able to achieve performance around 84% with an upper bound on accuracy for custom models at around 88%.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxgfUbeJ_8O6"
      },
      "source": [
        "**Baseline Neural Network Model Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfjopGE_VQ-K"
      },
      "source": [
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KheWqnyeAGFF"
      },
      "source": [
        "# load dataset\n",
        "dataframe = pd.read_csv(\"/content/sonar.all-data\", header=None)\n",
        "\n",
        "dataset = dataframe.values\n",
        "\n",
        "# split into input (X) and output (Y) variables\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y = dataset[:,60]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj2XP5UYAw9C"
      },
      "source": [
        "The output variable is string values. We must convert them into integer values 0 and 1.\n",
        "\n",
        "\n",
        "We can do this using the LabelEncoder class from scikit-learn. \n",
        "\n",
        "This class will model the encoding required using the entire dataset via the fit() function, then apply the encoding to create a new output variable using the transform() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7HdQ3tQAn77"
      },
      "source": [
        "# encode class values as integers\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNaZcTiAA-A0"
      },
      "source": [
        "We are now ready to create our neural network model using Keras.\n",
        "\n",
        "We are going to use scikit-learn to evaluate the model using stratified k-fold cross validation.\n",
        "\n",
        "This is a resampling technique that will provide an estimate of the performance of the model.\n",
        "\n",
        "**Explaination:** It does this by splitting the data into k-parts, training the model on all parts except one which is held out as a test set to evaluate the performance of the model. This process is repeated k-times and the average score across all constructed models is used as a robust estimate of performance. It is stratified, meaning that it will look at the output values and attempt to balance the number of instances that belong to each class in the k-splits of the data.\n",
        "\n",
        "Let’s start off by defining the function that creates our baseline model. \n",
        "\n",
        "Our model will have a single fully connected hidden layer with the same number of neurons as input variables. \n",
        "\n",
        "This is a good default starting point when creating neural networks.\n",
        "\n",
        "The Rectifier activation function is used. \n",
        "\n",
        "The output layer contains a single neuron in order to make predictions. \n",
        "\n",
        "It uses the sigmoid activation function in order to produce a probability output in the range of 0 to 1 that can easily and automatically be converted to crisp class values.\n",
        "\n",
        "Finally, we are using the logarithmic loss function (binary_crossentropy) during training, the preferred loss function for binary classification problems. \n",
        "\n",
        "The model also uses the efficient Adam optimization algorithm for gradient descent and accuracy metrics will be collected when the model is trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxMaFVrjA1XA"
      },
      "source": [
        "# baseline model\n",
        "def create_baseline():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(60, input_dim=60, activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LmmbIXc3B-YR",
        "outputId": "49dbe06f-8c32-47d5-e1da-1722ac130d28"
      },
      "source": [
        "# evaluate model with standardized dataset\n",
        "\n",
        "estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "results = cross_val_score(estimator, X, encoded_Y, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline: 81.71% (6.39%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50qTVRcHCZ3B"
      },
      "source": [
        "#Re-Run The Baseline Model With Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cMwN_JpCpk5"
      },
      "source": [
        "It is a good practice to prepare your data before modeling.\n",
        "\n",
        "Neural network models are especially suitable to having consistent input values, both in scale and distribution.\n",
        "\n",
        "An effective data preparation scheme for tabular data when building neural network models is standardization. \n",
        "\n",
        "This is where the data is rescaled such that the mean value for each attribute is 0 and the standard deviation is 1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgNLacKwEm_8"
      },
      "source": [
        "We can use scikit-learn to perform the standardization of our Sonar dataset using the StandardScaler class.\n",
        "\n",
        "\n",
        "Rather than performing the standardization on the entire dataset, it is good practice to train the standardization procedure on the training data within the pass of a cross-validation run and to use the trained standardization to prepare the “unseen” test fold. \n",
        "\n",
        "We can achieve this in scikit-learn using a Pipeline. The pipeline is a wrapper that executes one or more models within a pass of the cross-validation procedure. \n",
        "\n",
        "Here, we can define a pipeline with the StandardScaler followed by our neural network model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYPWU7ukCCO5",
        "outputId": "ca95d0eb-7092-498d-f1b0-85748c1ac58a"
      },
      "source": [
        "# evaluate baseline model with standardized dataset\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Standardized: 86.00% (9.97%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npb4VMqhGRWK"
      },
      "source": [
        "#Tuning Layers and Number of Neurons in The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDjKBQ8rGemR"
      },
      "source": [
        "**Evaluate a Smaller Network**\n",
        "\n",
        "We can suspect that there may be a lot of redundancy in the input variables for this problem.\n",
        "\n",
        "The data describes the same signal from different angles. \n",
        "\n",
        "Perhaps some of those angles are more relevant than others. \n",
        "\n",
        "We can force a type of feature extraction by the network by restricting the representational space in the first hidden layer.\n",
        "\n",
        "So we take our baseline model with 60 neurons in the hidden layer and reduce it by half to 30. \n",
        "\n",
        "This will put pressure on the network during training to pick out the most important structure in the input data to model.\n",
        "\n",
        "We will also standardize the data as in the previous experiment with data preparation and try to take advantage of the small lift in performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmflND2HFLk9",
        "outputId": "8ebacb3d-2097-4bdc-ba9c-a450c45f2071"
      },
      "source": [
        "# smaller model\n",
        "def create_smaller():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(30, input_dim=60, activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Smaller: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Smaller: 87.00% (7.84%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tS-ebCSrH3KM"
      },
      "source": [
        "**Evaluate a Larger Network**\n",
        "\n",
        "A neural network topology with more layers offers more opportunity for the network to extract key features and recombine them in useful nonlinear ways.\n",
        "\n",
        "We can evaluate whether adding more layers to the network improves the performance easily by making another small tweak to the function used to create our model. \n",
        "\n",
        "Here, we add one new layer (one line) to the network that introduces another hidden layer with 30 neurons after the first hidden layer.\n",
        "\n",
        "\n",
        "60 inputs -> [60 -> 30] -> 1 output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IslMvcI4G6g_",
        "outputId": "6a170f7a-c797-49d6-af19-abb8eae61cc1"
      },
      "source": [
        "# larger model\n",
        "def create_larger():\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(60, input_dim=60, activation='relu'))\n",
        "\tmodel.add(Dense(30, activation='relu'))\n",
        "\tmodel.add(Dense(1, activation='sigmoid'))\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\treturn model\n",
        "estimators = []\n",
        "estimators.append(('standardize', StandardScaler()))\n",
        "estimators.append(('mlp', KerasClassifier(build_fn=create_larger, epochs=100, batch_size=5, verbose=0)))\n",
        "pipeline = Pipeline(estimators)\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
        "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Larger: 87.10% (7.37%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qmz_IMPKMMcW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}